[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notes on Bayesian Stats",
    "section": "",
    "text": "Preface\nThese are some personal notes on Bayesian statistics.\nThe demos are mostly going to be in Julia, using the Turing.jl framework. But there may be other languages/frameworks used as well."
  },
  {
    "objectID": "ch1_intro.html",
    "href": "ch1_intro.html",
    "title": "1  Ch 1 - Intro",
    "section": "",
    "text": "2 Probability\nBayesian and frequentist frameworks tend to differ on what “probability” means. In a Bayesian framework, probability refers to the plausibility of an event. And this tends to be how most people use the term “probability” in informal settings. If we say that the Chiefs have a 90% probability of winning a game vs the Broncos, we’re probably using “probability” in the Bayesian sense.\nFrequentists, on the other hand, use probability to mean the relative frequency of a given event if it were repeated a lot of times. So in the above example, if this exact Chiefs team played this exact Broncos team in the same conditions 1,000 times, we’d expect them to win 900 of those games.\nIn a Bayesian framework, we ask this question: In light of the observed data, what’s the chance that the hypothesis is correct?\nIn a frequentist framework, we ask this question: If in fact the hypothesis is incorrect, what’s the chance I’d have observed this, or even more extreme, data?\n(note that both of the above are from Ch 1 of Bayes Rules!)\nUnconditional probability: \\(P(Y)\\) – the probability of X (e.g. the probability that an email is spam)\nConditional probability: \\(P(Y|X)\\) – the probability of Y given X (e.g. the probability that an email is spam given that there’s an exclamation mark in the subject line).\nIn some cases, \\(P(Y|X) &gt; P(Y)\\), for example \\(P(orchestra | practice) &gt; P(orchestra)\\), but in other cases, \\(P(Y|X) &lt; P(Y)\\), for example \\(P(flu | wash hands) &lt; P(flu)\\)\nOrdering is also important. Typically \\(P(Y|X) \\neq P(X|Y)\\).\nIndependence: two events are independent if \\(P(Y|X) = P(Y)\\).\nJoint probability: \\(P(Y \\cap X)\\) probabilty of Y and X. Assuming X is a binary variable, the total probability of observing Y is: \\(P(Y) = P(Y \\cap X) + P(Y \\cap X^c)\\), where \\(X^c\\) refers to “not X”\nFor events A and B, the posterior probability of B given A is:\n\\(P(B|A) = \\frac{P(A \\cap B)}{P(A)} = \\frac{P(B)L(B|A)}{P(A)}\\)\nwhere\n\\(P(A) = P(B)L(B|A) + P(B^c)L(B^c|A)\\)\nor more generally:\n\\(posterior = \\frac{prior \\cdot likelihood}{normalizing constant}\\)\nAnother way to think about this:\n\\(f(\\pi | y) = \\frac{f(\\pi)L(\\pi|y)}{f(y)}\\)\nusing RDatasets\nusing DataFrames\nusing Statistics\nusing Chain\n\ndefault = dataset(\"ISLR\", \"Default\")\n\n#we'll just use default and student for this\nd = default[:, [:Default, :Student]]\nd.:Default .= d.:Default .== \"Yes\"\nd.:Student .= d.:Student .== \"Yes\"\n\n10000-element BitVector:\n 0\n 1\n 0\n 0\n 0\n 1\n 0\n 1\n 0\n 0\n 1\n 1\n 0\n ⋮\n 0\n 1\n 0\n 0\n 0\n 0\n 1\n 0\n 0\n 0\n 0\n 1\nlet’s look at the overall probability of default\nmean(d.:Default)\n\n0.0333\nand the overall probability of being a student\nmean(d.:Student)\n\n0.2944\nand the probability of default by student type\np_tbl = @chain d begin\n    groupby(:Student)\n    combine(:Default =&gt; mean)\nend\n\n2×2 DataFrame\n\n\n\nRow\nStudent\nDefault_mean\n\n\n\nBool\nFloat64\n\n\n\n\n1\nfalse\n0.029195\n\n\n2\ntrue\n0.0431386\nso, the likelihoods for student types are, \\(L(S|D) = .043\\) and \\(L(S^c|D) = .029\\)\nIf we want to figure out the probability of default for students, we can use:\n\\(P(S|D) = \\frac{P(S)L(S|D)}{P(D)}\\)\np_s_d = (mean(d.:Student) * 0.043) / (mean(d.:Default))\n\n0.3801561561561561\nSo, given this, if we know someone defaults, there’s a 38% probability that they’re a student"
  },
  {
    "objectID": "ch1_intro.html#probability-vs-likelihood",
    "href": "ch1_intro.html#probability-vs-likelihood",
    "title": "1  Ch 1 - Intro",
    "section": "4.1 Probability vs Likelihood",
    "text": "4.1 Probability vs Likelihood\nWhen B is known, the conditional probability function \\(P(\\cdot|B)\\) allows us to compare the probabilities of an unknown event, A or \\(A^c\\), ocurring with B:\n\\(P(A|B)\\) vs \\(P(A^c|B)\\)\nWhen A is known, the likelihood function \\(L(\\cdot|A) = P(A|\\cdot)\\) allows us to evaluate the relative compatibility of data A with events B or \\(B^c\\):\n\\(L(B|A)\\) vs \\(L(B^c|A)\\).\nFor example, when Y = y is known, we can use a likelihood function (\\(L(\\cdot |y) = f(y|\\cdot)\\)) to compare the relative likelihood of observing data y under possible values of \\(\\pi\\) (in a binomial distribution), e.g. (\\(L(\\pi_1 | y)\\) vs \\(L(\\pi_2 | y)\\))."
  },
  {
    "objectID": "ch1_intro.html#calculating-joint-probability",
    "href": "ch1_intro.html#calculating-joint-probability",
    "title": "1  Ch 1 - Intro",
    "section": "4.2 Calculating Joint probability",
    "text": "4.2 Calculating Joint probability\n\\(P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\)"
  },
  {
    "objectID": "ch2_beta_binomial.html#beta-distribution",
    "href": "ch2_beta_binomial.html#beta-distribution",
    "title": "2  Ch 2 - The Beta Binomial Model",
    "section": "2.1 Beta Distribution",
    "text": "2.1 Beta Distribution\nThe beta distribution is parameterized by \\(\\alpha, \\beta &gt; 0\\). If a random variable, X, is beta-distributed, we can notate it like so:\n\\(X \\sim Beta(\\alpha, \\beta)\\)\nThe beta distribution can approximate a Normal distribution when \\(\\alpha \\sim \\beta\\) and \\(\\alpha\\) and \\(\\beta\\) &gt;&gt; 1.\nFor example\n\nusing Distributions\nusing Plots\nusing Random\nusing StatsPlots\n\nd = Beta(4, 4) #alpha = 4, beta = 4\nx = 0:.01:1\n\ny = pdf.(d, x)\n\n101-element Vector{Float64}:\n 0.0\n 0.00013584185999999964\n 0.0010541350399999993\n 0.0034499039399999987\n 0.007927234559999998\n 0.01500406249999997\n 0.025116860159999928\n 0.03862522313999997\n 0.0558163558399999\n 0.07690945625999977\n 0.10205999999999993\n 0.13136392345999992\n 0.1648617062399998\n ⋮\n 0.1313639234599997\n 0.10205999999999983\n 0.07690945625999984\n 0.055816355839999804\n 0.03862522313999984\n 0.025116860160000015\n 0.015004062500000024\n 0.007927234560000012\n 0.003449903940000005\n 0.001054135040000001\n 0.00013584185999999964\n 0.0\n\n\n\nplot(x, y, label=\"Beta(4, 4)\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbut the distribution obviously doesn’t have to look like this. We can change the shape by changing alpha and beta\n\nd2 = Beta(1.5, 4)\n\ny2 = pdf.(d2, x)\n\nplot!(x, y2, label = \"Beta(1.5, 4)\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nand again\n\nd3 = Beta(4, 1.5)\n\ny3 = pdf.(d3, x)\n\nplot!(x, y3, label = \"Beta(4, 1.5)\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne of the important things to remember, here, though, is that \\(0 \\le x \\le 1\\)\nWe can also make the beta distribution approximate the uniform distribution if \\(\\alpha = \\beta = 1\\)\n\nd4 = Beta(1, 1)\n\ny4 = pdf.(d4, x)\n\nplot!(x, y4, label = \"Beta(1,1)\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe flexibility of the beta distribution can make it useful."
  },
  {
    "objectID": "ch2_beta_binomial.html#binomial-distribution",
    "href": "ch2_beta_binomial.html#binomial-distribution",
    "title": "2  Ch 2 - The Beta Binomial Model",
    "section": "2.2 Binomial Distribution",
    "text": "2.2 Binomial Distribution\nThe binomial distribution is used when Y is a count outcome (e.g. the number of wins in a set of matches). Proportion outcomes are just rescaled count outcomes, so this distribution applies to proportions as well.\n\\(Y|\\pi \\sim Bin(n, \\pi)\\)\nwhere \\(\\pi\\) is the probability of success in a given trial.\nWe can plot this as well\n\nb = Binomial(100, .5) # 100 trials with π = .5\n\nx_bin = 0:1:100\n\ny_bin = pdf.(b, x_bin)\n\nplot(x_bin, y_bin, label=\"Bin(100, .5)\")"
  },
  {
    "objectID": "ch2_beta_binomial.html#the-beta-binomial-model",
    "href": "ch2_beta_binomial.html#the-beta-binomial-model",
    "title": "2  Ch 2 - The Beta Binomial Model",
    "section": "2.3 The Beta-Binomial Model",
    "text": "2.3 The Beta-Binomial Model\nThe components above are sufficient to describe our Beta-Binomial model:\n\\(Y|\\pi \\sim Bin(n, \\pi)\\) \\(\\pi \\sim Beta(\\alpha, \\beta)\\)"
  },
  {
    "objectID": "ch2_beta_binomial.html#simulate-the-beta-binomial-model",
    "href": "ch2_beta_binomial.html#simulate-the-beta-binomial-model",
    "title": "2  Ch 2 - The Beta Binomial Model",
    "section": "2.4 Simulate the Beta-Binomial model",
    "text": "2.4 Simulate the Beta-Binomial model\nLet’s say we want to predict the proportion of people who support Michelle in an election (basically the probability that she’ll win). We can simulate some data by sampling from the beta and binomial distributions.\nLet’s start by setting up a prior for our values of \\(\\pi\\). If we assume that our Beta distribution is parameterized as Beta(45, 55), we can simulate 1,000 values of \\(\\pi\\) from this distribution.\n\nRandom.seed!(0408)\n\nα = 45\nβ = 55\n\nd = Beta(α, β)\n\nn = 1_000 # sample 1k values\n\npi_sim = rand(d, n)\n\n#plot the distribution of pi values\ndensity(pi_sim)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThen, for each of these 1,000 values of \\(\\pi\\) we’ve simulated, let’s assume we poll 100 people, and the proportion of people who support Michelle follows a Binomial distribution such that Binomial(100, \\(\\pi\\)). We’ll draw one sample from each Binomial distribution (i.e. each value of \\(\\pi\\)), and then we can plot the distribution of our posterior.\n\n#simulate y\ny_sim = vcat(rand.(Binomial.(100, pi_sim), 1)...)\n\ndensity(y_sim)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis will show us, roughly, how many people (out of 100) we can expect to support Michelle.\nAs a further step, let’s assume our data suggested that the “true” value of y is 50 – i.e. that we conducted a poll and 50 (out of 100) people suggested they’d vote for Michelle. We can see the distribution of \\(\\pi\\) parameter values that produced these outcomes.\n\ninds = findall(x -&gt; x .== 50, y_sim)\n\npi_50 = pi_sim[inds]\n\ndensity(pi_50)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis would probably be better if we had more values in our sample, but we get the point."
  },
  {
    "objectID": "ch3_gamma_poisson.html#gamma-distribution",
    "href": "ch3_gamma_poisson.html#gamma-distribution",
    "title": "3  Ch 3 - The Gamma-Poisson Model",
    "section": "3.1 Gamma Distribution",
    "text": "3.1 Gamma Distribution\nThe Gamma Distribution will serve as our prior for \\(\\lambda\\). We can notate this via:\n\\(\\lambda \\sim Gamme(s, r)\\)\nThe Gamma Distribution is parameterized by a shape parameter (s) and a rate parameter (r). These have the constraint that s, r &gt; 0.\nNote that the Exponential Distribution is a special case of the Gamma Distribution where s = 1.\nIn general, these distributions are positive and right-skewed (see this figure, for example).\nTo set an informative prior, we need to choose reasonable values of s and r. If we assume that we receive 5 phone calls in a day, we can estimate s and r using the equation \\(E(\\lambda) = \\frac{s}{r} \\approx 5\\), so we know that \\(s = 5r\\). We can then plot some distributions that satisfy this and choose some values.\nNote that the Distributions.jl package parameterizes the Gamma distribution with shape (s, \\(\\alpha\\)) and a scale parameter (\\(\\theta\\)), which is the inverse of the rate (r, \\(\\beta\\)) parameter (i.e. \\(\\beta = 1 / \\theta\\))\n\nr = 1:4\nθ = 1 ./ r\ns = 5 .* r\n\ngammas = Gamma.(s, θ)\n\nplots = []\nfor Γ in gammas\n    α = Γ.α\n    θ = round(Γ.θ, digits=2)\n    y = rand(Γ, 10_000)\n    p = density(y, label=\"α = $α; θ = $θ\")\n    push!(plots, p)\nend\n\nplot!(plots...)"
  },
  {
    "objectID": "ch3_gamma_poisson.html#the-gamma-poisson-model",
    "href": "ch3_gamma_poisson.html#the-gamma-poisson-model",
    "title": "3  Ch 3 - The Gamma-Poisson Model",
    "section": "3.2 The Gamma-Poisson Model",
    "text": "3.2 The Gamma-Poisson Model\nGiven the above, the Gamma-Poisson model is described as:\n\\(Y_i|\\lambda \\sim Pois(\\lambda)\\) \\(\\lambda \\sim Gamma(s, r)\\)"
  },
  {
    "objectID": "ch4_posterior_simulation.html",
    "href": "ch4_posterior_simulation.html",
    "title": "4  Ch 4 - Posterior Simulation",
    "section": "",
    "text": "5 Grid approximation\nImagine we have the following beta-binomial model:\n\\(Y|\\pi \\sim Bin(10, \\pi)\\) \\(\\pi \\sim Beta(2, 2)\\)\nwhere Y is the number of successes in 10 independent trials. Each trial has probability \\(\\pi\\) of succeeding. Imagine that we observe Y = 9 successes.\nAlthough we can specify the posterior analytically, we can also approximate it with grid approximation.\nusing Distributions\nusing Plots\nusing Random\nusing StatsPlots\nusing StatsBase\n\nRandom.seed!(0408)\n\n#vector of potential pi values\npi_grid = 0:0.1:1\n\n#prior beta distribution\nd_beta = Beta(2, 2)\n\nBeta{Float64}(α=2.0, β=2.0)\n#evaluate the pdf at each value of pi\npriors = pdf.(d_beta, pi_grid)\n\n#calculate the likelihood at each value of pi\nlikelihoodz = pdf.(Binomial.(10, pi_grid), 9)\n\n#compute the un-normalized posterior\nun_normalized = likelihoodz .* priors\n\n#normalize the posterior\nposteriorz = un_normalized ./ sum(un_normalized)\n\n11-element Vector{Float64}:\n 0.0\n 6.99703248673802e-9\n 5.661203963590892e-6\n 0.00024994099745876834\n 0.0032608534830283545\n 0.02108962820317927\n 0.08357210821151959\n 0.2196098326613785\n 0.3710126629578925\n 0.30119930528454697\n 0.0\nAnd then we can plot the resulting grid-approximated posterior\nplot(pi_grid, posteriorz)\nscatter!(pi_grid, posteriorz)\nOf course, if our pi_grid contains more closely-spaced values, the approximated posterior becomes smoother:\npi2 = 0:0.01:1\nprior2 = pdf.(d_beta, pi2)\n\n#calculate the likelihood at each value of pi\nlikelihood2 = pdf.(Binomial.(10, pi2), 9)\n\n#compute the un-normalized posterior\nun_normalized2 = likelihood2 .* prior2\n\n#normalize the posterior\nposterior2 = un_normalized2 ./ sum(un_normalized2)\n\nplot(pi2, posterior2)\nscatter!(pi2, posterior2)\nWe can add the true resulting distribution – a Beta(11, 3) distribution – to this to show the approximation vs the true value:\ns = sample(pi2, Weights(posterior2), 100_000)\n\ndensity(s, label=\"Approximate Posterior\")\n\ntrue_beta = Beta(11, 3)\nys = pdf.(true_beta, pi2)\nplot!(pi2, ys, label=\"True Beta(11,3)\")\nOne issue with grid approximation is that it quickly becomes intractable with many parameters – it is difficult to get a dense enough grid to get a good approximation as you start adding parameters.\nSo here’s MCMC to the rescue!\nLike grid approximation models, MCMC samples are approximations.\nUnlike grid approximation samples, though, MCMC samples are not independent. For \\({\\theta^1, \\theta^2, ..., \\theta^N}\\), each \\(\\theta^{(i+1)}\\) is conditional on \\(\\theta^i\\). Or:\n\\(f(\\theta^{(i+1)} | \\theta^i, y)\\)"
  },
  {
    "objectID": "ch4_posterior_simulation.html#metropolis-hastings-algorithm",
    "href": "ch4_posterior_simulation.html#metropolis-hastings-algorithm",
    "title": "4  Ch 4 - Posterior Simulation",
    "section": "6.1 Metropolis-Hastings algorithm",
    "text": "6.1 Metropolis-Hastings algorithm\nThe Metropolis-Hastings algo is a 2-step process for approximating a posterior distribution:\n\nPropose a new “location” in the theoretical posterior;\nDecide whether or not to go there.\n\nTo unpack this a little bit:\nStep 1: Propose a new location\nWe don’t know the posterior we’re drawing from, but we can use a “proposal model” to draw new locations from. We can use different distributions, but the Uniform distribution is the most straightforward. Assume we have a current location, \\(x\\), and bandwidth, \\(w\\), then our proposed new location, \\(x'\\), is:\n\\(x'|x \\sim Unif(x - w, x + w)\\)\nStep 2: Decide whether or not to go there\nWe have to calculate the acceptance probability (the probability of accepting \\(x'\\)).\nIf the unnormalized posterior plausibility of the proposed location \\(x'\\) is greather than that of the current location \\(x\\) (i.e. \\(f(x')L(x'|y) &gt; f(x)L(x|y)\\)), definitely go to \\(x'\\).\nOtherwise, maybe go to \\(x'\\). This feature (maybe moving even if the proposed location is less likely than the current location) prevents the chain from getting stuck.\nWe can define the acceptance probability as:\n\\[\\alpha = min\\{1, \\frac{f(x')L(x'|y)}{f(x)L(x|y)} \\frac{q(x|x')}{q(x'|x)}\\}\\]\nIf we use a symmetric proposal model (like the Uniform distribution), then this simplifies to:\n\\[\\alpha = min\\{1, \\frac{f(x')L(x'|y)}{f(x)L(x|y)}\\}\\]\nThis can further simplify to:\n\\[\\alpha = min\\{1, \\frac{f(x'|y)}{f(x|y)}\\}\\]\nGiven this, in scenario 1, if \\(f(x'|y) \\ge f(x|y)\\), then $alpha = 1 $, and we will definitely move to \\(x'\\)\nOtherwise, if \\(\\alpha = \\frac{f(x'|y)}{f(x|y)} \\lt 1\\), then we might move there with probability \\(\\alpha\\)\n\n6.1.1 example\nImagine we have a Normal-Normal model such that:\n\\(Y|\\mu \\sim N(\\mu, 0.75^2)\\) \\(\\mu \\sim N(0, 1^2)\\)\ni.e. Y is a numerical outcome that varies normally around \\(\\mu\\) with standard deviation 0.75\n\nfunction one_mh_iter(w, current)\n    u = Uniform(current - w, current + w)\n    proposal = rand(u, 1)[1]\n\n    proposal_plaus = pdf(Normal(0, 1), proposal) * pdf(Normal(proposal, 0.75), 6.25)\n    current_plaus = pdf(Normal(0, 1), current) * pdf(Normal(current, 0.75), 6.25)\n\n    α = minimum([1, proposal_plaus / current_plaus])\n\n    next = sample([proposal, current], Weights([α, α - 1]), 1)[1]\n\n    r = [current, proposal, α, next]\n    return r\nend\n\none_mh_iter (generic function with 1 method)\n\n\nRESUME HERE – CHECK THE ABOVE\n\nres = one_mh_iter(1, 4)\n\n4-element Vector{Float64}:\n 4.0\n 3.4419156420806463\n 0.6488324482870187\n 3.4419156420806463"
  },
  {
    "objectID": "distributions.html#binomial-distribution",
    "href": "distributions.html#binomial-distribution",
    "title": "5  Appendix - Distributions",
    "section": "5.1 Binomial Distribution",
    "text": "5.1 Binomial Distribution\nUsed when Y is a count outcome (e.g. the number of wins in a set of matches)\n\\(Y|\\pi \\sim Bin(n, \\pi)\\)\nwhere \\(\\pi\\) is the probability of success in a given trial"
  }
]