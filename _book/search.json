[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notes on Bayesian Stats",
    "section": "",
    "text": "Preface\nThese are some personal notes on Bayesian statistics.\nThe demos are mostly going to be in Julia, using the Turing.jl framework. But there may be other languages/frameworks used as well.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "ch1_intro.html",
    "href": "ch1_intro.html",
    "title": "1  Ch 1 - Intro",
    "section": "",
    "text": "2 Probability\nBayesian and frequentist frameworks tend to differ on what “probability” means. In a Bayesian framework, probability refers to the plausibility of an event. And this tends to be how most people use the term “probability” in informal settings. If we say that the Chiefs have a 90% probability of winning a game vs the Broncos, we’re probably using “probability” in the Bayesian sense.\nFrequentists, on the other hand, use probability to mean the relative frequency of a given event if it were repeated a lot of times. So in the above example, if this exact Chiefs team played this exact Broncos team in the same conditions 1,000 times, we’d expect them to win 900 of those games.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Ch 1 - Intro</span>"
    ]
  },
  {
    "objectID": "ch1_intro.html#probability-vs-likelihood",
    "href": "ch1_intro.html#probability-vs-likelihood",
    "title": "1  Ch 1 - Intro",
    "section": "4.1 Probability vs Likelihood",
    "text": "4.1 Probability vs Likelihood\nWhen B is known, the conditional probability function \\(P(\\cdot|B)\\) allows us to compare the probabilities of an unknown event, A or \\(A^c\\), ocurring with B:\n\\(P(A|B)\\) vs \\(P(A^c|B)\\)\nWhen A is known, the likelihood function \\(L(\\cdot|A) = P(A|\\cdot)\\) allows us to evaluate the relative compatibility of data A with events B or \\(B^c\\):\n\\(L(B|A)\\) vs \\(L(B^c|A)\\).\nFor example, when Y = y is known, we can use a likelihood function (\\(L(\\cdot |y) = f(y|\\cdot)\\)) to compare the relative likelihood of observing data y under possible values of \\(\\pi\\) (in a binomial distribution), e.g. (\\(L(\\pi_1 | y)\\) vs \\(L(\\pi_2 | y)\\)).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Ch 1 - Intro</span>"
    ]
  },
  {
    "objectID": "ch1_intro.html#calculating-joint-probability",
    "href": "ch1_intro.html#calculating-joint-probability",
    "title": "1  Ch 1 - Intro",
    "section": "4.2 Calculating Joint probability",
    "text": "4.2 Calculating Joint probability\n\\(P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Ch 1 - Intro</span>"
    ]
  },
  {
    "objectID": "ch2_beta_binomial.html",
    "href": "ch2_beta_binomial.html",
    "title": "2  Ch 2 - The Beta Binomial Model",
    "section": "",
    "text": "2.1 Beta Distribution\nThe beta distribution is parameterized by \\(\\alpha, \\beta &gt; 0\\). If a random variable, X, is beta-distributed, we can notate it like so:\n\\(X \\sim Beta(\\alpha, \\beta)\\)\nThe beta distribution can approximate a Normal distribution when \\(\\alpha \\sim \\beta\\) and \\(\\alpha\\) and \\(\\beta\\) &gt;&gt; 1.\nFor example\nusing Distributions\nusing Plots\nusing Random\nusing StatsPlots\n\nd = Beta(4, 4) #alpha = 4, beta = 4\nx = 0:.01:1\n\ny = pdf.(d, x)\n\n101-element Vector{Float64}:\n 0.0\n 0.00013584185999999964\n 0.0010541350399999993\n 0.0034499039399999987\n 0.007927234559999998\n 0.01500406249999997\n 0.025116860159999928\n 0.03862522313999997\n 0.0558163558399999\n 0.07690945625999977\n 0.10205999999999993\n 0.13136392345999992\n 0.1648617062399998\n ⋮\n 0.1313639234599997\n 0.10205999999999983\n 0.07690945625999984\n 0.055816355839999804\n 0.03862522313999984\n 0.025116860160000015\n 0.015004062500000024\n 0.007927234560000012\n 0.003449903940000005\n 0.001054135040000001\n 0.00013584185999999964\n 0.0\nplot(x, y, label=\"Beta(4, 4)\")\nbut the distribution obviously doesn’t have to look like this. We can change the shape by changing alpha and beta\nd2 = Beta(1.5, 4)\n\ny2 = pdf.(d2, x)\n\nplot!(x, y2, label = \"Beta(1.5, 4)\")\nand again\nd3 = Beta(4, 1.5)\n\ny3 = pdf.(d3, x)\n\nplot!(x, y3, label = \"Beta(4, 1.5)\")\nOne of the important things to remember, here, though, is that \\(0 \\le x \\le 1\\)\nWe can also make the beta distribution approximate the uniform distribution if \\(\\alpha = \\beta = 1\\)\nd4 = Beta(1, 1)\n\ny4 = pdf.(d4, x)\n\nplot!(x, y4, label = \"Beta(1,1)\")\nThe flexibility of the beta distribution can make it useful.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Ch 2 - The Beta Binomial Model</span>"
    ]
  },
  {
    "objectID": "ch2_beta_binomial.html#binomial-distribution",
    "href": "ch2_beta_binomial.html#binomial-distribution",
    "title": "2  Ch 2 - The Beta Binomial Model",
    "section": "2.2 Binomial Distribution",
    "text": "2.2 Binomial Distribution\nThe binomial distribution is used when Y is a count outcome (e.g. the number of wins in a set of matches). Proportion outcomes are just rescaled count outcomes, so this distribution applies to proportions as well.\n\\(Y|\\pi \\sim Bin(n, \\pi)\\)\nwhere \\(\\pi\\) is the probability of success in a given trial.\nWe can plot this as well\n\nb = Binomial(100, .5) # 100 trials with π = .5\n\nx_bin = 0:1:100\n\ny_bin = pdf.(b, x_bin)\n\nplot(x_bin, y_bin, label=\"Bin(100, .5)\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Ch 2 - The Beta Binomial Model</span>"
    ]
  },
  {
    "objectID": "ch2_beta_binomial.html#the-beta-binomial-model",
    "href": "ch2_beta_binomial.html#the-beta-binomial-model",
    "title": "2  Ch 2 - The Beta Binomial Model",
    "section": "2.3 The Beta-Binomial Model",
    "text": "2.3 The Beta-Binomial Model\nThe components above are sufficient to describe our Beta-Binomial model:\n\\(Y|\\pi \\sim Bin(n, \\pi)\\) \\(\\pi \\sim Beta(\\alpha, \\beta)\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Ch 2 - The Beta Binomial Model</span>"
    ]
  },
  {
    "objectID": "ch2_beta_binomial.html#simulate-the-beta-binomial-model",
    "href": "ch2_beta_binomial.html#simulate-the-beta-binomial-model",
    "title": "2  Ch 2 - The Beta Binomial Model",
    "section": "2.4 Simulate the Beta-Binomial model",
    "text": "2.4 Simulate the Beta-Binomial model\nLet’s say we want to predict the proportion of people who support Michelle in an election (basically the probability that she’ll win). We can simulate some data by sampling from the beta and binomial distributions.\nLet’s start by setting up a prior for our values of \\(\\pi\\). If we assume that our Beta distribution is parameterized as Beta(45, 55), we can simulate 1,000 values of \\(\\pi\\) from this distribution.\n\nRandom.seed!(0408)\n\nα = 45\nβ = 55\n\nd = Beta(α, β)\n\nn = 1_000 # sample 1k values\n\npi_sim = rand(d, n)\n\n#plot the distribution of pi values\ndensity(pi_sim)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThen, for each of these 1,000 values of \\(\\pi\\) we’ve simulated, let’s assume we poll 100 people, and the proportion of people who support Michelle follows a Binomial distribution such that Binomial(100, \\(\\pi\\)). We’ll draw one sample from each Binomial distribution (i.e. each value of \\(\\pi\\)), and then we can plot the distribution of our posterior.\n\n#simulate y\ny_sim = vcat(rand.(Binomial.(100, pi_sim), 1)...)\n\ndensity(y_sim)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis will show us, roughly, how many people (out of 100) we can expect to support Michelle.\nAs a further step, let’s assume our data suggested that the “true” value of y is 50 – i.e. that we conducted a poll and 50 (out of 100) people suggested they’d vote for Michelle. We can see the distribution of \\(\\pi\\) parameter values that produced these outcomes.\n\ninds = findall(x -&gt; x .== 50, y_sim)\n\npi_50 = pi_sim[inds]\n\ndensity(pi_50)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis would probably be better if we had more values in our sample, but we get the point.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Ch 2 - The Beta Binomial Model</span>"
    ]
  },
  {
    "objectID": "ch3_gamma_poisson.html",
    "href": "ch3_gamma_poisson.html",
    "title": "3  Ch 3 - The Gamma-Poisson Model",
    "section": "",
    "text": "3.1 Gamma Distribution\nThe Gamma Distribution will serve as our prior for \\(\\lambda\\). We can notate this via:\n\\(\\lambda \\sim Gamme(s, r)\\)\nThe Gamma Distribution is parameterized by a shape parameter (s) and a rate parameter (r). These have the constraint that s, r &gt; 0.\nNote that the Exponential Distribution is a special case of the Gamma Distribution where s = 1.\nIn general, these distributions are positive and right-skewed (see this figure, for example).\nTo set an informative prior, we need to choose reasonable values of s and r. If we assume that we receive 5 phone calls in a day, we can estimate s and r using the equation \\(E(\\lambda) = \\frac{s}{r} \\approx 5\\), so we know that \\(s = 5r\\). We can then plot some distributions that satisfy this and choose some values.\nNote that the Distributions.jl package parameterizes the Gamma distribution with shape (s, \\(\\alpha\\)) and a scale parameter (\\(\\theta\\)), which is the inverse of the rate (r, \\(\\beta\\)) parameter (i.e. \\(\\beta = 1 / \\theta\\))\nr = 1:4\nθ = 1 ./ r\ns = 5 .* r\n\ngammas = Gamma.(s, θ)\n\nplots = []\nfor Γ in gammas\n    α = Γ.α\n    θ = round(Γ.θ, digits=2)\n    y = rand(Γ, 10_000)\n    p = density(y, label=\"α = $α; θ = $θ\")\n    push!(plots, p)\nend\n\nplot!(plots...)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Ch 3 - The Gamma-Poisson Model</span>"
    ]
  },
  {
    "objectID": "ch3_gamma_poisson.html#the-gamma-poisson-model",
    "href": "ch3_gamma_poisson.html#the-gamma-poisson-model",
    "title": "3  Ch 3 - The Gamma-Poisson Model",
    "section": "3.2 The Gamma-Poisson Model",
    "text": "3.2 The Gamma-Poisson Model\nGiven the above, the Gamma-Poisson model is described as:\n\\(Y_i|\\lambda \\sim Pois(\\lambda)\\) \\(\\lambda \\sim Gamma(s, r)\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Ch 3 - The Gamma-Poisson Model</span>"
    ]
  },
  {
    "objectID": "ch4_posterior_simulation.html",
    "href": "ch4_posterior_simulation.html",
    "title": "4  Ch 4 - Posterior Simulation",
    "section": "",
    "text": "4.1 Grid approximation\nImagine we have the following beta-binomial model:\n\\(Y|\\pi \\sim Bin(10, \\pi)\\) \\(\\pi \\sim Beta(2, 2)\\)\nwhere Y is the number of successes in 10 independent trials. Each trial has probability \\(\\pi\\) of succeeding. Imagine that we observe Y = 9 successes.\nAlthough we can specify the posterior analytically, we can also approximate it with grid approximation.\nusing Distributions\nusing Plots\nusing Random\nusing StatsPlots\nusing StatsBase\n\nRandom.seed!(0408)\n\n#vector of potential pi values\npi_grid = 0:0.1:1\n\n#prior beta distribution\nd_beta = Beta(2, 2)\n\nBeta{Float64}(α=2.0, β=2.0)\n#evaluate the pdf at each value of pi\npriors = pdf.(d_beta, pi_grid)\n\n#calculate the likelihood at each value of pi\nlikelihoodz = pdf.(Binomial.(10, pi_grid), 9)\n\n#compute the un-normalized posterior\nun_normalized = likelihoodz .* priors\n\n#normalize the posterior\nposteriorz = un_normalized ./ sum(un_normalized)\n\n11-element Vector{Float64}:\n 0.0\n 6.99703248673802e-9\n 5.661203963590892e-6\n 0.00024994099745876834\n 0.0032608534830283545\n 0.02108962820317927\n 0.08357210821151959\n 0.2196098326613785\n 0.3710126629578925\n 0.30119930528454697\n 0.0\nAnd then we can plot the resulting grid-approximated posterior\nplot(pi_grid, posteriorz)\nscatter!(pi_grid, posteriorz)\nOf course, if our pi_grid contains more closely-spaced values, the approximated posterior becomes smoother:\npi2 = 0:0.01:1\nprior2 = pdf.(d_beta, pi2)\n\n#calculate the likelihood at each value of pi\nlikelihood2 = pdf.(Binomial.(10, pi2), 9)\n\n#compute the un-normalized posterior\nun_normalized2 = likelihood2 .* prior2\n\n#normalize the posterior\nposterior2 = un_normalized2 ./ sum(un_normalized2)\n\nplot(pi2, posterior2)\nscatter!(pi2, posterior2)\nWe can add the true resulting distribution – a Beta(11, 3) distribution – to this to show the approximation vs the true value:\ns = sample(pi2, Weights(posterior2), 100_000)\n\ndensity(s, label=\"Approximate Posterior\")\n\ntrue_beta = Beta(11, 3)\nys = pdf.(true_beta, pi2)\nplot!(pi2, ys, label=\"True Beta(11,3)\")\nOne issue with grid approximation is that it quickly becomes intractable with many parameters – it is difficult to get a dense enough grid to get a good approximation as you start adding parameters.\nSo here’s MCMC to the rescue!",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ch 4 - Posterior Simulation</span>"
    ]
  },
  {
    "objectID": "ch4_posterior_simulation.html#mcmc",
    "href": "ch4_posterior_simulation.html#mcmc",
    "title": "4  Ch 4 - Posterior Simulation",
    "section": "4.2 MCMC",
    "text": "4.2 MCMC\nLike grid approximation models, MCMC samples are approximations.\nUnlike grid approximation samples, though, MCMC samples are not independent. For \\({\\theta^1, \\theta^2, ..., \\theta^N}\\), each \\(\\theta^{(i+1)}\\) is conditional on \\(\\theta^i\\). Or:\n\\(f(\\theta^{(i+1)} | \\theta^i, y)\\)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ch 4 - Posterior Simulation</span>"
    ]
  },
  {
    "objectID": "ch4_posterior_simulation.html#metropolis-hastings-algorithm",
    "href": "ch4_posterior_simulation.html#metropolis-hastings-algorithm",
    "title": "4  Ch 4 - Posterior Simulation",
    "section": "4.3 Metropolis-Hastings algorithm",
    "text": "4.3 Metropolis-Hastings algorithm\nThe Metropolis-Hastings algo is a 2-step process for approximating a posterior distribution:\n\nPropose a new “location” in the theoretical posterior;\nDecide whether or not to go there.\n\nTo unpack this a little bit:\nStep 1: Propose a new location\nWe don’t know the posterior we’re drawing from, but we can use a “proposal model” to draw new locations from. We can use different distributions, but the Uniform distribution is the most straightforward. Assume we have a current location, \\(x\\), and bandwidth, \\(w\\), then our proposed new location, \\(x'\\), is:\n\\(x'|x \\sim Unif(x - w, x + w)\\)\nStep 2: Decide whether or not to go there\nWe have to calculate the acceptance probability (the probability of accepting \\(x'\\)).\nIf the unnormalized posterior plausibility of the proposed location \\(x'\\) is greather than that of the current location \\(x\\) (i.e. \\(f(x')L(x'|y) &gt; f(x)L(x|y)\\)), definitely go to \\(x'\\).\nOtherwise, maybe go to \\(x'\\). This feature (maybe moving even if the proposed location is less likely than the current location) prevents the chain from getting stuck.\nWe can define the acceptance probability as:\n\\[\\alpha = min\\{1, \\frac{f(x')L(x'|y)}{f(x)L(x|y)} \\frac{q(x|x')}{q(x'|x)}\\}\\]\nIf we use a symmetric proposal model (like the Uniform distribution), then this simplifies to:\n\\[\\alpha = min\\{1, \\frac{f(x')L(x'|y)}{f(x)L(x|y)}\\}\\]\nThis can further simplify to:\n\\[\\alpha = min\\{1, \\frac{f(x'|y)}{f(x|y)}\\}\\]\nGiven this, in scenario 1, if \\(f(x'|y) \\ge f(x|y)\\), then $alpha = 1 $, and we will definitely move to \\(x'\\)\nOtherwise, if \\(\\alpha = \\frac{f(x'|y)}{f(x|y)} \\lt 1\\), then we might move there with probability \\(\\alpha\\)\n\n4.3.1 example\nImagine we have a Normal-Normal model such that:\n\\(Y|\\mu \\sim N(\\mu, 0.75^2)\\)\n\\(\\mu \\sim N(0, 1^2)\\)\ni.e. Y is a numerical outcome that varies normally around \\(\\mu\\) with standard deviation 0.75\nThen imagine we observe an outcome Y = 6.25. Given this, and the above, we can calculate the posterior model as:\n\\(\\mu | (Y = 6.25) \\sim N(4, 0.6^2)\\)\nWe can run through one iteration of the Metropolis-Hastings algorithm with the following, where w is the bandwidth of our Uniform proposal model and current is the current value on the chain:\n\nfunction one_mh_iter(w, current)\n    c = Float64(current)\n    u = Uniform(current - w, current + w)\n    proposal = rand(u, 1)[1]\n\n    #proposal_plaus = prior * likelihood\n    proposal_plaus = pdf(Normal(0, 1), proposal) * pdf(Normal(proposal, 0.75), 6.25)\n\n    #current_plaus = prior * likelihood\n    current_plaus = pdf(Normal(0, 1), current) * pdf(Normal(current, 0.75), 6.25)\n\n    α = minimum([1, proposal_plaus / current_plaus])\n\n    next = sample([proposal, c], Weights([α, 1 - α]), 1)[1]\n\n    return next\nend\n\none_mh_iter (generic function with 1 method)\n\n\n\nres = one_mh_iter(1, 4)\n\n4.0\n\n\nAnd then from there, we can use the logic in the one_mh_iter function to take a “tour” and create an entire chain, which will be our simulated distribution.\n\nfunction mh_tour(w, current, N::Int)\n    μ = Vector{Float64}(undef, N)\n\n    for i ∈ eachindex(μ)\n        r = one_mh_iter(w, current)\n        μ[i] = r\n        current = r\n    end\n    return μ\nend\n\nmh_tour (generic function with 1 method)\n\n\n\nres = mh_tour(1, 4, 10_000)\n\n10000-element Vector{Float64}:\n 4.0\n 4.106609027850608\n 4.235886932047645\n 3.310894252707528\n 4.034179768273493\n 4.512960695766677\n 4.512960695766677\n 4.349575562580946\n 4.101991567803039\n 3.460601626178285\n 3.633816936072791\n 4.30401065365775\n 3.401923900064334\n ⋮\n 3.934350716176937\n 3.934350716176937\n 3.66719175153303\n 3.9773404547954994\n 3.7463512503491367\n 4.521828790801184\n 4.521828790801184\n 4.492908196169627\n 3.8668258667074396\n 3.8668258667074396\n 3.8668258667074396\n 3.8668258667074396\n\n\nand we can look at the trace plot and the histogram to see the distribution we’ve simulated:\n\np1 = plot(res, label=\"\");\np2 = histogram(res);\n\nplot(p1, p2, layout=@layout([a b]))\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd now let’s check out work – we should get approximately \\(\\mu = 4\\) and \\(\\sigma = 0.6\\)\n\n[mean(res), std(res)]\n\n2-element Vector{Float64}:\n 3.974150267286955\n 0.6134780589942376",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ch 4 - Posterior Simulation</span>"
    ]
  },
  {
    "objectID": "ch4_posterior_simulation.html#other-algorithms",
    "href": "ch4_posterior_simulation.html#other-algorithms",
    "title": "4  Ch 4 - Posterior Simulation",
    "section": "4.4 Other Algorithms",
    "text": "4.4 Other Algorithms\nMetropolis-Hastings is the fundamental MCMC algorithm, but it’s often not the one used in applications today. Other algorithms, such as the Gibbs algorithm or the Hamiltonian Monte Carlo (HMC) algorithm are more common, with HMC being the algorithm used by the {rstan} package.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ch 4 - Posterior Simulation</span>"
    ]
  },
  {
    "objectID": "distributions.html",
    "href": "distributions.html",
    "title": "5  Appendix - Distributions",
    "section": "",
    "text": "5.1 Binomial Distribution\nUsed when Y is a count outcome (e.g. the number of wins in a set of matches)\n\\(Y|\\pi \\sim Bin(n, \\pi)\\)\nwhere \\(\\pi\\) is the probability of success in a given trial",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Appendix - Distributions</span>"
    ]
  },
  {
    "objectID": "distributions.html#multivariate-normal",
    "href": "distributions.html#multivariate-normal",
    "title": "5  Appendix - Distributions",
    "section": "5.2 Multivariate Normal",
    "text": "5.2 Multivariate Normal\nA multivariate normal distribution is an abstraction of the univariate normal distribution. It’s parameterized by two components:\n\na mean vector, \\(\\mu\\), and;\na covariance matrix, \\(\\Sigma\\)\n\nThe diagonal of the covariance matrix describes each variable’s (e.g. \\(x_i\\)) variance, whereas all off-diagonal elements describe the covariance between, \\(x_i\\) and \\(x_j\\) or whatever you want to refer to the variables as.\nIf the off-diagonal elements are all 0, then all of the variables are independent. The code below shows an example of a multivariate normal distribution with 3 independent variables, all with a mean of 0 and a variance of 5.\n\nusing Distributions\nusing LinearAlgebra\n\np = 3\n\nd = MvNormal(zeros(p), 5.0 * I)\n\nIsoNormal(\ndim: 3\nμ: [0.0, 0.0, 0.0]\nΣ: [5.0 0.0 0.0; 0.0 5.0 0.0; 0.0 0.0 5.0]\n)\n\n\nAnd the code below will do create a multivariate normal distribution where the variables are correlated\n\nΣ = [[1.0, 0.8, 0.7] [0.8, 1.0, 0.9] [0.7, 0.9, 1.0]]\n\nd2 = MvNormal(zeros(p), Σ)\n\nFullNormal(\ndim: 3\nμ: [0.0, 0.0, 0.0]\nΣ: [1.0 0.8 0.7; 0.8 1.0 0.9; 0.7 0.9 1.0]\n)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Appendix - Distributions</span>"
    ]
  }
]