---
title: "Ch 5 - Linear Regression"
jupyter: julia-1.9
---

We can further apply Bayesian statistics to help with estimating the parameters of a linear regression model. Here, I'm going to use Julia's [Turing.jl](https://turinglang.org/) package to fit the model.

```{julia}
using Distributions
using Plots
using Turing
using LinearAlgebra
using Random
using StatsPlots
using GLM

Random.seed!(0408)
```

## Generate Fake Data

I'll generate some fake data to use as an example.

```{julia}

n = 1000

ùêó = randn(n, 3)

Œ≤ = [1.0, 2.0, 3.0]

f(x) = 0.5 .+ x * Œ≤

œµ = rand(Normal(0, 1.0), n)

y = f(ùêó) + œµ;
```


Here's what we're modeling:

$Y_i \sim N(\alpha + \beta^T \bf X_i, \sigma^2)$

In words, $Y_i$ is normally distributed with an expected value of the output of a typical linear model and a variance of $\sigma^2$.

## Define Model

Next, we can use Turing's `@model macro` to define our model. One important part of defining our model is setting priors.

In this model, I'm going to use pretty weak priors, and I'm going to use these same priors for all of the variables.

```{julia}
@model function lin_reg(x, y)
    n_feat = size(x, 2)

    #priors
    Œ± ~ Normal(0, 2)
    b ~ MvNormal(zeros(n_feat), 3.0 * I)
    œÉ ~ Exponential(1)

    #estimate the likelihood
    for i ‚àà eachindex(y)
        y[i] ~ Normal(Œ± + x[i, :]' * b, œÉ)
    end

    #alternatively:
    # Œº = Œ± .+ x * b
    # return y ~ MvNormal(Œº, œÉ * I)
end
```

In math notation, our model is:

$Y_i | \alpha, \beta, \sigma \sim N(\alpha + \bf X_i ' \beta, \sigma^2)$

$\alpha \sim N(0, 2)$

$\beta \sim N(\mu, \Sigma)$

where $\mu$ is a length-3 zero-vector and $\Sigma$ is a diagonal matrix with 3s on the diagonal

$\sigma \sim Exp(1)$

## Simulate the Posterior

Ok so now we have this model set up, which is what a fairly typical linear regression model might look like, and now we need to simulate the posterior. We can do that in Turing as follows: 

```{julia}
model = lin_reg(ùêó, y)

chn = sample(model, NUTS(), MCMCThreads(), 5_000, 2);
```

And we can check the trace plot and the posterior distributions:

```{julia}
plot(chn)
```

These plots give some useful diagnostics. The trace plots show how the chains (in blue and yellow) are moving around the range of posterior plausible values for each parameter, and the distributions show the posterior distributions for each parameter. Since we know the true values for these parameters, we can see that simulations are producing posteriors representative of the ground truth.

## Extract Information from Posterior

We can also extract summary information from our simulation. We might care about the parameter means (we probably also care about their distributions, but more on that next) and other diagnostics, which we can access via `summarize()`. 

```{julia}
summarize(chn)
```

In addition to providing parameter means, this function also gives us the `rhat` values for each parameter. `rhat` describes the ratio of the variability across all chains to the variability within any individual chain. We want this to be approximately 1, and values greater than like 1.05 might be concerning.

Note that the formula here is:

$\hat{R} \approx \sqrt{\frac{V_{combined}}{V_{within}}}$

Another thing we might want to do is extract quantiles from our chain. In a Bayesian context, we probably care more about quantiles/distributions than just parameter means, otherwise like why bother to adopt a Bayesian approach at all?

```{julia}
quantile(chn)
```

By default, `quantile()` provides the 2.5%, 25%, 50%, 75%, and 97.5% percentiles. If we wanted, for example, 80% confidence for each parameter, we could supply this:

```{julia}
quantile(chn; q=[0.1, 0.9])
```

and if we wanted to get the quantiles for each chain separately:

```{julia}
qs = quantile(chn; q=[0.1, 0.9], append_chains=false)

#and let's look at just the first one
qs[1]
```

## Predicting Y 

So, probably obvious, but another thing we're probably interested in doing is predicting our outcome. In a Bayesian framework, though, these predictions won't be a single value -- they'll be distributions. Or, rather, simulated/approximate distributions.

The way this works is that, for each observation of $y$, we'll get $j$ predictions, where $j$ is the number of parameter sets across the chains. Since we have 2 chains, each with 5,000 iterations here, we'll get 10,000 predictions for each observation of $y$ here.

In notation:

$Y_i | \alpha^j, \beta^j, \sigma^j \sim N(\alpha^j + \bf X_i ' \beta^j, \sigma{^j}^2)$

where $j$ is a given parameter set.

So in our current case, where we have 1,000 observations of $y$ and 2 length-5,000 chains, we will have $1,000*2*5,000 = 10,000,000$ predicted values.

In Turing, we can get those like so:

```{julia}
yÃÇ = Vector{Union{Missing,Float64}}(undef, length(y))

preds = predict(lin_reg(ùêó, yÃÇ), chn);

```

Now, if we want to extract predictions for just our first observation (y1) and plot the distribution of predictions, we can do:

```{julia}
y1 = getindex(preds, "y[1]")

density(y1.data)
```

And we can get the mean prediction for each observation in the data if we want:

```{julia}
mean_preds = summarize(preds)[:, :mean]
```

## Compare to OLS

Since we simulated our data, we know what our true parameter values are, and so we can just kinda look at the parameter estimates from the Bayesian framework and see that they're very close to the true values. But it could also be instructive to compare the parameter estimates -- and the predicted values -- to those we get from OLS.

So let's first get our mean parameter estimates from the Bayesian framework:

```{julia}
bayes_coefs = summarize(chn)[:, :mean]
```

Then we can get our OLS coefficients using Julia's linear solver notation (we could also use the `GLM` package).

```{julia}
ols_coefs = hcat(ones(length(y)), ùêó) \ y
```

And we can see that they're basically the same with the `isapprox()` function.

```{julia}
isapprox(ols_coefs, bayes_coefs[1:4], atol=0.01)
```

Finally, let's say we wanted to compare the predictions of the Bayesian model to those of the OLS model. We can calculate the mean squared error for each.

We don't *really* need to do this, since if the parameters are essentially the same, we know the model predictions will also be essentially the same, but it's easy to do so whatever.

```{julia}
yÃÇ_ols = hcat(ones(length(y)), ùêó) * ols_coefs;

yÃÇ_bayes = summarize(preds)[:, :mean];

function mse(y, yÃÇ)
    return sum((y .- yÃÇ) .^ 2) / length(y)
end
```

```{julia}
mse_ols = round(mse(y, yÃÇ_ols), digits=2);
mse_bayes = round(mse(y, yÃÇ_bayes), digits=2);

print("OLS loss: $mse_ols \nBayes loss: $mse_bayes")
```

So there we go -- the whole process of fitting a Bayesian linear regression!